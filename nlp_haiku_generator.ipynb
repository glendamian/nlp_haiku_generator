{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098d1fd6",
   "metadata": {},
   "source": [
    "# DS 4400 Final Project : Haiku Generator\n",
    "\n",
    "#### Ben Tunney, Glen Damian Lim\n",
    "\n",
    "#### Datasets : https://www.kaggle.com/datasets/hjhalani30/haiku-dataset (English haikus)\n",
    "\n",
    "#### NLP models: N-gram Language Model, Recurrent Neural Network\n",
    "\n",
    "###### INSTRUCTIONS: To run this notebook, you have to run it in the order of the top cell to bottom cell. When running the main() function for the dashboard, it will return a link that will lead you to your localhost in order to run the dashboard locally. All of the required libraries and an external \".py\" file for our ngram model are listed/imported in this notebook, please contact one of us if you run into any problems and thanks you for a great semester!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1620bfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import syllables\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('cmudict')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Neural Networks libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Separate file provided for the ngram model, make sure to put in the same directory\n",
    "import ngram_model as ngm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d989",
   "metadata": {},
   "source": [
    "### Getting data and text pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac92e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and get data\n",
    "def get_haiku_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    sentences = df['0'] + ' ' + df['1'] + ' ' + df['2'] + ' ' \n",
    "    data = [str(sentence).split() for sentence in sentences]\n",
    "    return data\n",
    "\n",
    "# lemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "# Checks if given word contains a special character\n",
    "def contains_special(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric() or (not char.isalnum()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# process tokens\n",
    "def process_tokens(toks):\n",
    "    toks = [lm.lemmatize(word.lower()) for word in toks \n",
    "          # make sure no strings that contain only numeric characters \n",
    "          if not contains_special(word)]\n",
    "    return toks\n",
    "\n",
    "# Read and pre-process our data\n",
    "def read_haikus(data, ngram):\n",
    "    result = []\n",
    "    for sentences in data:\n",
    "        toks = nltk.word_tokenize(' '.join([word for word in sentences]))\n",
    "        processed = process_tokens(toks)\n",
    "        if len(processed) != 0 and len(processed) < 17:\n",
    "            processed = ['<h>'] * (ngram-1) + processed + ['</h>'] * (ngram-1)\n",
    "            result.append(processed)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2668657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_haiku_data('all_haiku.csv')\n",
    "# Get haikus data with trigram\n",
    "haikus = read_haikus(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20741bd",
   "metadata": {},
   "source": [
    "### Training word embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1a14b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 32328\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 200\n",
    "\n",
    "def train_embeddings(data):\n",
    "    return Word2Vec(sentences=haikus, vector_size=embedding_size, window=5, min_count=1, \n",
    "                 sg=1)\n",
    "    \n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "word2vec_model = train_embeddings(haikus)\n",
    "vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "print('Vocab size {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec3d2f",
   "metadata": {},
   "source": [
    "# N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c41efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find haikus that are similar\n",
    "def find_similar_haikus(haikus, inputs, embeddings):\n",
    "    \"\"\"Find haikus that contain words from the given inputs\n",
    "    Parameters:\n",
    "      haikus (list): list of list of processed haikus tokens\n",
    "      inputs (list): list of words to match\n",
    "      embeddings (Word2Vec): trained word embeddings\n",
    "\n",
    "    Returns:\n",
    "      list: list of list of processed haikus tokens that contain words from the given inputs\n",
    "    \"\"\"\n",
    "    similar_words = []\n",
    "    for word in inputs:\n",
    "        # Find top 5 similar words to current word\n",
    "        find_similar = [similar_words.append(w) for w,s in embeddings.wv.most_similar(word, topn=5)]\n",
    "    training_haikus = []\n",
    "    for haiku in haikus:\n",
    "        if any(word in haiku for word in similar_words):\n",
    "            training_haikus.append(haiku)\n",
    "    return [\" \".join(haiku) for haiku in training_haikus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e45dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " now we where we\n",
      " your story of the playoff this\n",
      " playoff looking like a\n",
      "\n",
      "\n",
      " am i the only\n",
      " and people wonder why i\n",
      " out football season\n",
      "\n",
      "\n",
      " football is on and\n",
      " been telling it not fun to\n",
      " gutted to hear this\n",
      "\n",
      "\n",
      " targeting is the\n",
      " how do the hockey night on\n",
      " getting ready for\n",
      "\n",
      "\n",
      " not even close\n",
      " football practice the coach\n",
      " i ca fucking stand\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_haikus = find_similar_haikus(haikus, ['basketball'], word2vec_model)\n",
    "\n",
    "# Define new N-gram Language Model object\n",
    "ngram_lm = ngm.LanguageModel(3, True, line_begin=\"<\" + \"h\" + \">\", line_end=\"</\" + \"h\" + \">\")\n",
    "# Training the model with haikus similar to inputs\n",
    "ngram_lm.train(similar_haikus)\n",
    "\n",
    "# Generate 5 haikus with the query input 'basketball'\n",
    "for haiku in ngram_lm.generate_haiku(5):\n",
    "    for line in haiku:\n",
    "        print(line)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03372a01",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d86dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(model, tokenizer):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    word_to_index = tokenizer.word_index\n",
    "\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        embedding = model.wv[word]\n",
    "        word_to_embedding[word] = embedding\n",
    "        index_to_embedding[word_to_index[word]] = embedding\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n",
    "def padded_data(encoded, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for row in encoded:\n",
    "        for i in range(1, len(row) - 1):\n",
    "            X.append(row[:i])\n",
    "            y.append(row[i])\n",
    "    X = pad_sequences(X, maxlen = seq_length - 1)\n",
    "    return X, y\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, vocab_size: int, index_to_embedding: dict) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    '''\n",
    "    # inputs\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        end_index = i + num_sequences_per_batch\n",
    "        # if we ran out of data\n",
    "        if end_index >= len(X) - 1:\n",
    "            i = 0\n",
    "            end_index = i + num_sequences_per_batch\n",
    "        \n",
    "        inputs = [val for val in X[i:end_index]]\n",
    "        # outputs into one hot encoding\n",
    "        outputs = [to_categorical(val, vocab_size, dtype = 'int32') for val in y[i:end_index]]\n",
    "        yield np.array(inputs), np.array(outputs)\n",
    "        i += num_sequences_per_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c2c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# Get haikus data with unigram\n",
    "haikus = read_haikus(data, 1)\n",
    "# Train word embeddings with data vocabulary\n",
    "word2vec_model = train_embeddings(haikus)\n",
    "vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "tokenizer.fit_on_texts(haikus)\n",
    "\n",
    "# Embeddings\n",
    "word_to_embedding, index_to_embedding = read_embeddings(word2vec_model, tokenizer)\n",
    "# Embedding for zero index\n",
    "index_to_embedding[0] = np.zeros((embedding_size,))\n",
    "word_to_embedding[''] = np.zeros((embedding_size,))\n",
    "vocab_size = len(word_to_embedding.keys())\n",
    "\n",
    "# Encode words into index\n",
    "encoded = tokenizer.texts_to_sequences(haikus)\n",
    "seq_length = max([len(sequence) for sequence in encoded])\n",
    "# Performs pre-padding on training data with a sliding window approach\n",
    "X_encoded, y = padded_data(encoded, seq_length)\n",
    "\n",
    "# Convert X into 3D (num_instances, sequence length, embedding_size)\n",
    "X = np.zeros((len(X_encoded), seq_length - 1, embedding_size))\n",
    "for i in range(X_encoded.shape[0]):\n",
    "    for j in range(X_encoded.shape[1]):\n",
    "        word = X_encoded[i,j]\n",
    "        X[i, j, :] = index_to_embedding[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad5f829-202a-4124-9f3a-0b4c2d3da2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5369/5369 [==============================] - 1050s 196ms/step - loss: 6.0456 - accuracy: 0.1010\n",
      "Epoch 2/5\n",
      "5369/5369 [==============================] - 1053s 196ms/step - loss: 5.5328 - accuracy: 0.1373\n",
      "Epoch 3/5\n",
      "5369/5369 [==============================] - 1118s 208ms/step - loss: 5.3457 - accuracy: 0.1481\n",
      "Epoch 4/5\n",
      "5369/5369 [==============================] - 1016s 189ms/step - loss: 5.2298 - accuracy: 0.1543\n",
      "Epoch 5/5\n",
      "5369/5369 [==============================] - 950s 177ms/step - loss: 5.1460 - accuracy: 0.1584\n",
      "Tensor(\"dense_1/Softmax:0\", shape=(None, 32327), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Start training the model\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "num_sequences_per_batch = 256\n",
    "steps_per_epoch = len(X)//num_sequences_per_batch\n",
    "\n",
    "# Data generator\n",
    "train_generator = data_generator(X, y, num_sequences_per_batch, vocab_size, index_to_embedding)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "# LSTM input layer\n",
    "model.add(LSTM(128, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n",
    "# Dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "# Bidirectional LSTM layer for extra context\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x= train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=num_epochs, verbose = 1)\n",
    "\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ee7a226-c6df-46d8-86a3-a51e15029154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 syllable_limit: int):\n",
    "    ''' Generate a line from the model based on the given syllable limit\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        syllable_limit: generate a sentence of length n syllable\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    query = seed[0]\n",
    "    sentence = [query]\n",
    "    count_syllables = syllables.estimate(query)\n",
    "    \n",
    "    # while current syllable is not the limit\n",
    "    while count_syllables != syllable_limit:\n",
    "        # n-1 tokens in sentence\n",
    "        curr_tokens = sentence\n",
    "        # encode our tokens\n",
    "        sequence = tokenizer.texts_to_sequences([curr_tokens])[0]\n",
    "        # pre-padding our tokens\n",
    "        sequence = np.array(pad_sequences([sequence], maxlen = seq_length-1, padding='pre'))\n",
    "        # Convert into 3D in order to feed to NN for prediction\n",
    "        embeddings = np.zeros((sequence.shape[0], sequence.shape[1], embedding_size))\n",
    "        for i in range(sequence.shape[0]):\n",
    "            for j in range(sequence.shape[1]):\n",
    "                word = sequence[i,j]\n",
    "                embeddings[i, j, :] = index_to_embedding[word]\n",
    "    \n",
    "        # get probability distribution\n",
    "        probs = model.predict(embeddings)[0]\n",
    "        # normalize probabilities and get index\n",
    "        random_choice = np.random.choice(len(probs),p = probs / np.sum(probs))\n",
    "        next_word = tokenizer.index_word[random_choice]\n",
    "        # Count new syllables\n",
    "        new_count = syllables.estimate(next_word) + count_syllables\n",
    "        \n",
    "        # if next word is not haiku begin or end token and under syllable limit\n",
    "        if next_word not in ['<h>','</h>'] and (new_count <= syllable_limit):\n",
    "            sentence.append(next_word)\n",
    "            count_syllables = new_count\n",
    "        else:\n",
    "            # restart until we find matching syllable\n",
    "            sentence = [query]\n",
    "            count_syllables = syllables.estimate(query)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "062507a8-e61f-43ef-980c-c5d5d362e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_haiku(n, query):\n",
    "    \"\"\"Generates n haikus from a trained language model\n",
    "    Parameters:\n",
    "      n (int): the number of haikus to generate\n",
    "\n",
    "    Returns:\n",
    "      list: a list containing strings, one per generated sentence\n",
    "    \"\"\"\n",
    "    haikus = []\n",
    "    while n > 0:\n",
    "        haiku = []\n",
    "        # Generate each line of haikus with 5-7-5 syllable limit pattern\n",
    "        line_1 = generate_line(model, tokenizer, [query], 5)\n",
    "        line_2 = generate_line(model, tokenizer, [line_1.split()[-1]], 7)\n",
    "        line_3 = generate_line(model, tokenizer, [line_2.split()[-1]], 5)\n",
    "        haiku.append(line_1)\n",
    "        haiku.append(line_2)\n",
    "        haiku.append(line_3)\n",
    "        haikus.append(haiku)\n",
    "        n -= 1\n",
    "    return haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a90276f3-475f-449d-88e9-62e6f1bdad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wind might be making\n",
      "making someone big health\n",
      "health is the best part\n",
      "\n",
      "\n",
      "wind have all bitter\n",
      "bitter horse moon group of\n",
      "of course that be\n",
      "\n",
      "\n",
      "wind by the first of\n",
      "of course maybe now i\n",
      "i want to get so\n",
      "\n",
      "\n",
      "wind work no bear at\n",
      "at least why i want a card\n",
      "card if your skill is\n",
      "\n",
      "\n",
      "wind birthday story\n",
      "story like it so hard to\n",
      "to stay at checking\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for haiku in generate_haiku(5, 'wind'):\n",
    "    for line in haiku:\n",
    "        print(line)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63208708-6f4a-454b-b367-612201493589",
   "metadata": {},
   "source": [
    "# Plotly Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf08625-9ee6-4991-988f-748a1099b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for ai image\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "\n",
    "# libraries for ner\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac13cb05-9e17-4cda-bd64-6bc0043296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"\"  # token in case you want to use private API\n",
    "headers = {\n",
    "    # \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"X-Wait-For-Model\": \"true\",\n",
    "    \"X-Use-Cache\": \"false\"\n",
    "}\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5\"\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return Image.open(io.BytesIO(response.content))\n",
    "\n",
    "\n",
    "def slugify(text):\n",
    "    # remove non-word characters and foreign characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \"-\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7db8ede-b608-4864-85e8-b6befe764060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(prompt, mdl):\n",
    "\n",
    "    # https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n",
    "    text1 = NER(prompt)\n",
    "\n",
    "    # try to get NERs\n",
    "    ners = \"\"\n",
    "    for word in text1.ents:\n",
    "        ner = word.text\n",
    "        ners += ner \n",
    "        ners += \" \"\n",
    "        \n",
    "    # if there are NERs, use for prompt\n",
    "    if ners != \"\":\n",
    "        prompt = ners\n",
    "        \n",
    "    # used stopword-removed haiku as prompt\n",
    "    else:\n",
    "        tokenized = prompt.split(\" \")\n",
    "        wordsList = [w for w in tokenized if w not in stop_words]\n",
    "        prompt = \" \".join(wordsList)\n",
    "\n",
    "    # save img\n",
    "    image = query({\"inputs\": prompt})\n",
    "    image.save(f\"image_{mdl}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbbe0340-ffc9-4e68-bd78-9e90c6030c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_haikus(word, model):\n",
    "    if model == 'ngram':\n",
    "        similar_haikus = find_similar_haikus(haikus, [word], word2vec_model)\n",
    "\n",
    "        # Define new N-gram Language Model object\n",
    "        ngram_lm = ngm.LanguageModel(2, True, line_begin=\"<\" + \"h\" + \">\", line_end=\"</\" + \"h\" + \">\")\n",
    "\n",
    "        # Training the model with haikus similar to inputs\n",
    "        ngram_lm.train(similar_haikus)\n",
    "\n",
    "        haiku = ngram_lm.generate_haiku(1)\n",
    "    elif model == \"rnn\":\n",
    "        haiku = generate_haiku(1, word)\n",
    "    return haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b22016f-6db3-448b-abe4-a3ada95a192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # make app\n",
    "    external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "    # stylesheet with the .dbc class\n",
    "    dbc_css = \"https://cdn.jsdelivr.net/gh/AnnMarieW/dash-bootstrap-templates/dbc.min.css\"\n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.JOURNAL, dbc.themes.BOOTSTRAP, dbc_css])\n",
    "\n",
    "    channel_input = dcc.Input(\n",
    "        id=\"input-value\",\n",
    "        type=\"text\",\n",
    "        value=\"\",\n",
    "        size=\"lg\",\n",
    "        style={\"font-size\": \"1.6rem\", \"margin-top\": \".5px\"},\n",
    "        className=\"mb-3\"\n",
    "    )\n",
    "    button = dbc.Button(\n",
    "        id=\"search-button\",\n",
    "        children=\"Search\",\n",
    "        n_clicks=0,\n",
    "        size=\"lg\",\n",
    "        style={\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\"},\n",
    "        color=\"primary\",\n",
    "        className=\"me-1\",\n",
    "    )\n",
    "\n",
    "    header = html.H1(\"NN and N-Gram Haiku Generator\",\n",
    "                     style={\"margin-top\": \"50px\"})\n",
    "\n",
    "    caption = html.H6(\"Generate Haikus by Topic\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "    \n",
    "    ngram = html.H6(\"N-Gram Generated Haiku\",\n",
    "                      style={\"margin-top\": \"10px\", \"font-weight\": \"bold\"})\n",
    "    \n",
    "    lstm = html.H6(\"LSTM Generated Haiku\",\n",
    "                      style={\"margin-top\": \"10px\", \"font-weight\": \"bold\"})\n",
    "\n",
    "    five1 = html.H6(id= \"firstline\",\n",
    "                    children=\"11111111\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    seven1 = html.H6(id= \"secondline\",\n",
    "                    children=\"2222222222\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five2 = html.H6(id= \"thirdline\",\n",
    "                    children=\"33333333333\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five12 = html.H6(id= \"firstline2\",\n",
    "                    children=\"11111111\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    seven12 = html.H6(id= \"secondline2\",\n",
    "                    children=\"2222222222\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five22 = html.H6(id= \"thirdline2\",\n",
    "                    children=\"33333333333\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "\n",
    "    img1 = html.Img(id= \"image1\", src=\"\")\n",
    "    img2 = html.Img(id= \"image2\", src=\"\")\n",
    "\n",
    "    collapse1 = html.Div(\n",
    "        [\n",
    "            dbc.Collapse(\n",
    "                dbc.Card(dbc.CardBody([ngram, html.Div(className='gap'),five1, html.Div(className='gap'), seven1, html.Div(className='gap'), five2\n",
    "                                       , html.Div(className='gap'), img1])),\n",
    "                id=\"collapse1\",\n",
    "                is_open=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    collapse2 = html.Div(\n",
    "        [\n",
    "            dbc.Collapse(\n",
    "                dbc.Card(dbc.CardBody([lstm, html.Div(className='gap'),five12, html.Div(className='gap'), seven12, html.Div(className='gap'), five22\n",
    "                                       , html.Div(className='gap'), img2])),\n",
    "                id=\"collapse2\",\n",
    "                is_open=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    app.layout = dbc.Container(\n",
    "        [\n",
    "\n",
    "        # top line of Dash\n",
    "        dbc.Row([\n",
    "            dbc.Col(\n",
    "                [header,caption, channel_input, button, collapse1, collapse2],\n",
    "                \n",
    "                lg=6\n",
    "            )\n",
    "        ],\n",
    "            justify = \"center\",\n",
    "            style = dict(textAlign=\"center\"),\n",
    "            className=\"d-flex justify-content-center\",\n",
    "        ),],\n",
    "        className=\"p-4\",\n",
    "        fluid = True)\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"search-button\", \"style\"),\n",
    "        Input(\"input-value\", \"value\"),\n",
    "    )\n",
    "    def change_button_color(channel_input):\n",
    "        if channel_input != \"\":\n",
    "            return {\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\", \"background-color\": \"red\"}\n",
    "        else:\n",
    "            return {\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\", 'background-color': 'gray'}\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"search-button\", \"n_clicks\"),\n",
    "        Output('firstline', 'children'),\n",
    "        Output('secondline', 'children'),\n",
    "        Output('thirdline', 'children'),\n",
    "        Output('image1', 'src'),\n",
    "        Output('firstline2', 'children'),\n",
    "        Output('secondline2', 'children'),\n",
    "        Output('thirdline2', 'children'),\n",
    "        Output('image2', 'src'),\n",
    "        Input(\"search-button\", \"n_clicks\"),\n",
    "        Input(\"input-value\", \"value\"),\n",
    "    )\n",
    "\n",
    "    def init_countdown_store(n_clicks, search_results):\n",
    "        # Ngram model result\n",
    "        lines = [\"\", \"\", \"\"]\n",
    "        imgsrc_ngram = \"\"\n",
    "        if n_clicks > 0:\n",
    "            lines = return_haikus(search_results, \"ngram\")[0]\n",
    "            s = lines[0]+ lines[1]+ lines[2]\n",
    "            get_image(s, \"ngram\")\n",
    "            test_base64_ngram = base64.b64encode(open(\"image_ngram.png\", 'rb').read()).decode('ascii')\n",
    "            imgsrc_ngram = 'data:image/png;base64,{}'.format(test_base64_ngram)\n",
    "            \n",
    "        # RNN model result\n",
    "        lines2 = [\"\", \"\", \"\"]\n",
    "        imgsrc_lstm = \"\"\n",
    "        if n_clicks > 0:\n",
    "            lines2 = return_haikus(search_results, \"rnn\")[0]\n",
    "            s = lines[0]+ lines[1]+ lines[2]\n",
    "            get_image(s, \"lstm\")\n",
    "            test_base64_lstm = base64.b64encode(open(\"image_lstm.png\", 'rb').read()).decode('ascii')\n",
    "            imgsrc_lstm = 'data:image/png;base64,{}'.format(test_base64_ngram)\n",
    "        return 0, lines[0], lines[1], lines[2], imgsrc_ngram, lines2[0], lines2[1], lines2[2], imgsrc_lstm\n",
    "\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a755f-a03a-452e-bc22-fa9277911f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run server\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
