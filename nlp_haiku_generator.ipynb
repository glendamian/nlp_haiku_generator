{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DS 4400 Final Project : Haiku Generator\n\n#### Ben Tunney, Glen Damian Lim\n\n#### Datasets : https://www.kaggle.com/datasets/hjhalani30/haiku-dataset (English haikus)\n\n#### Word Embeddings: GloVe from https://nlp.stanford.edu/projects/glove/ (choose Wikipedia 2014 + Gigaword 5)\n\n#### NLP models: N-gram Language Model, Recurrent Neural Network, Transformers","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n# NLP libraries\nimport nltk\nfrom nltk.corpus import stopwords, cmudict\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\n\n# Neural Networks libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.models import Sequential\n\n# Outside Files\n# import ngram_model as ngm","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:48:25.843077Z","iopub.execute_input":"2023-04-20T00:48:25.843513Z","iopub.status.idle":"2023-04-20T00:48:33.859589Z","shell.execute_reply.started":"2023-04-20T00:48:25.843473Z","shell.execute_reply":"2023-04-20T00:48:33.858410Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Getting data and text pre-processing ","metadata":{}},{"cell_type":"code","source":"# Read file and get data\ndef get_haiku_data(fname):\n    df = pd.read_csv(fname)\n    sentences = df['0'] + ' ' + df['1'] + ' ' + df['2'] + ' ' \n    data = [str(sentence).split() for sentence in sentences]\n    return data\n\n# lemmatizer\nlm = WordNetLemmatizer()\n\ndef contains_special(word):\n    for char in word:\n        if char.isnumeric() or (not char.isalnum()):\n            return True\n    return False\n\n# process tokens\ndef process_tokens(toks):\n    toks = [lm.lemmatize(word.lower()) for word in toks \n          # make sure no strings that contain only numeric characters \n          if not contains_special(word)]\n    return toks\n\ndef read_haikus(data, ngram):\n    result = []\n    for sentences in data:\n        toks = nltk.word_tokenize(' '.join([word for word in sentences]))\n        processed = process_tokens(toks)\n        if len(processed) != 0 and len(processed) < 17:\n            processed = ['<h>'] * (ngram-1) + processed + ['</h>'] * (ngram-1)\n            result.append(processed)\n    return result\n\n# create an instance of the CMUDict\nsyllable = cmudict.dict()\ndef estimate_syllables(word):\n    try:\n        count = [len(list(y for y in x if y[-1].isdigit())) for x in syllable[word.lower()]]\n        return count\n    except KeyError:\n        return 100\n    \n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:48:33.861714Z","iopub.execute_input":"2023-04-20T00:48:33.862416Z","iopub.status.idle":"2023-04-20T00:48:36.222470Z","shell.execute_reply.started":"2023-04-20T00:48:33.862383Z","shell.execute_reply":"2023-04-20T00:48:36.221195Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"data = get_haiku_data('/kaggle/input/haiku-dataset/all_haiku.csv')\n# Get haikus data with trigram\n# haikus = read_haikus(data, 3)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:48:36.225179Z","iopub.execute_input":"2023-04-20T00:48:36.225614Z","iopub.status.idle":"2023-04-20T00:48:37.359152Z","shell.execute_reply.started":"2023-04-20T00:48:36.225567Z","shell.execute_reply":"2023-04-20T00:48:37.357870Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Training word embeddings using Word2Vec","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nembedding_size = 200\n\ndef train_embeddings(data):\n    return Word2Vec(sentences=haikus, vector_size=embedding_size, window=5, min_count=1, \n                 sg=1)\n    \n\n# # Train the Word2Vec model from Gensim. \n# word2vec_model = train_embeddings(haikus)\n# vocab_size = len(word2vec_model.wv.index_to_key)\n# print('Vocab size {}'.format(vocab_size))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:49:49.456844Z","iopub.execute_input":"2023-04-20T00:49:49.457345Z","iopub.status.idle":"2023-04-20T00:49:49.588146Z","shell.execute_reply.started":"2023-04-20T00:49:49.457301Z","shell.execute_reply":"2023-04-20T00:49:49.587128Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# N-gram Language Model","metadata":{}},{"cell_type":"code","source":"# Find haikus that are similar\ndef find_similar_haikus(haikus, inputs, embeddings):\n    \"\"\"Find haikus that contain words from the given inputs\n    Parameters:\n      haikus (list): list of list of processed haikus tokens\n      inputs (list): list of words to match\n      embeddings (Word2Vec): trained word embeddings\n\n    Returns:\n      list: list of list of processed haikus tokens that contain words from the given inputs\n    \"\"\"\n    similar_words = []\n    for word in inputs:\n        # Find top 5 similar words to current word\n        find_similar = [similar_words.append(w) for w,s in embeddings.wv.most_similar(word, topn=5)]\n    training_haikus = []\n    for haiku in haikus:\n        if any(word in haiku for word in similar_words):\n            training_haikus.append(haiku)\n    return [\" \".join(haiku) for haiku in training_haikus]","metadata":{"execution":{"iopub.status.busy":"2023-04-19T23:47:43.878245Z","iopub.execute_input":"2023-04-19T23:47:43.878955Z","iopub.status.idle":"2023-04-19T23:47:43.893655Z","shell.execute_reply.started":"2023-04-19T23:47:43.878914Z","shell.execute_reply":"2023-04-19T23:47:43.892637Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"similar_haikus = find_similar_haikus(haikus, ['basketball'], word2vec_model)\n\n# Define new N-gram Language Model object\nngram_lm = ngm.LanguageModel(3, True, line_begin=\"<\" + \"h\" + \">\", line_end=\"</\" + \"h\" + \">\")\n# Training the model with haikus similar to inputs\nngram_lm.train(similar_haikus)\n\nfor haiku in ngram_lm.generate_haiku(5):\n    for line in haiku:\n        print(line)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T23:47:43.898677Z","iopub.execute_input":"2023-04-19T23:47:43.899838Z","iopub.status.idle":"2023-04-19T23:47:44.944133Z","shell.execute_reply.started":"2023-04-19T23:47:43.899763Z","shell.execute_reply":"2023-04-19T23:47:44.942523Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2216091501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define new N-gram Language Model object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mngram_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_begin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"h\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\">\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"</\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"h\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Training the model with haikus similar to inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mngram_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_haikus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ngm' is not defined"],"ename":"NameError","evalue":"name 'ngm' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Recurrent Neural Networks (LSTMs)","metadata":{}},{"cell_type":"code","source":"def read_embeddings(model, tokenizer):\n    '''Loads and parses embeddings trained in earlier.\n    Parameters and return values are up to you.\n    '''\n    vocab = list(model.wv.index_to_key)\n    word_to_index = tokenizer.word_index\n\n    word_to_embedding = {}\n    index_to_embedding = {}\n\n    for word in vocab:\n        embedding = model.wv[word]\n        word_to_embedding[word] = embedding\n        index_to_embedding[word_to_index[word]] = embedding\n    return word_to_embedding, index_to_embedding\n\n# Produced pre-padded data for LSTM network\ndef padded_data(encoded, seq_length):\n    X = []\n    y = []\n    for row in encoded:\n        for i in range(1, len(row) - 1):\n            X.append(row[:i])\n            y.append(row[i])\n    X = pad_sequences(X, maxlen = seq_length - 1)\n    return X, y\n\ndef data_generator(X: list, y: list, num_sequences_per_batch: int, vocab_size: int, index_to_embedding: dict) -> (list,list):\n    '''\n    Returns data generator to be used by feed_forward\n    https://wiki.python.org/moin/Generators\n    https://realpython.com/introduction-to-python-generators/\n    \n    Yields batches of embeddings and labels to go with them.\n    Use one hot vectors to encode the labels \n    (see the to_categorical function)\n\n    '''\n    # inputs\n    i = 0\n    while i < len(X):\n        end_index = i + num_sequences_per_batch\n        # if we ran out of data\n        if end_index >= len(X) - 1:\n            i = 0\n            end_index = i + num_sequences_per_batch\n        \n        inputs = [val for val in X[i:end_index]]\n        # outputs into one hot encoding\n        outputs = [to_categorical(val, vocab_size, dtype = 'int32') for val in y[i:end_index]]\n        yield np.array(inputs), np.array(outputs)\n        i += num_sequences_per_batch","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:48:55.485098Z","iopub.execute_input":"2023-04-20T00:48:55.485678Z","iopub.status.idle":"2023-04-20T00:48:55.498852Z","shell.execute_reply.started":"2023-04-20T00:48:55.485642Z","shell.execute_reply":"2023-04-20T00:48:55.496372Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import math\ntokenizer = Tokenizer()\nhaikus = read_haikus(data, 1)\n# Using 50% of training data due to limited RAM\nhaikus = haikus[:math.floor(len(haikus) * 0.50)]\nword2vec_model = train_embeddings(haikus)\nvocab_size = len(word2vec_model.wv.index_to_key)\ntokenizer.fit_on_texts(haikus)\n\n# Embeddings\nword_to_embedding, index_to_embedding = read_embeddings(word2vec_model, tokenizer)\n# Embedding for zero index\nindex_to_embedding[0] = np.zeros((embedding_size,))\nword_to_embedding[''] = np.zeros((embedding_size,))\nvocab_size = len(word_to_embedding.keys())\n\n# Encode words into index\nencoded = tokenizer.texts_to_sequences(haikus)\nseq_length = 10\n# Padded data along with sliding window\nX_encoded, y = padded_data(encoded, seq_length)\n\n# Convert X into 3D (num_instances, sequence length, embedding_size)\nX = np.zeros((len(X_encoded), seq_length - 1, embedding_size))\nfor i in range(X_encoded.shape[0]):\n    for j in range(X_encoded.shape[1]):\n        word = X_encoded[i,j]\n        X[i, j, :] = index_to_embedding[word]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:50:04.285882Z","iopub.execute_input":"2023-04-20T00:50:04.286964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training the model\n\n# hyperparameters\nnum_epochs = 10\nnum_sequences_per_batch = 128\nsteps_per_epoch = len(encoded)//num_sequences_per_batch\n\n# Data generator\ntrain_generator = data_generator(X,y, num_sequences_per_batch, vocab_size, index_to_embedding)\n\nmodel = Sequential()\n# LSTM layer\nmodel.add(LSTM(512, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n# Dropout layer to prevent overfitting\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(vocab_size, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nmodel.fit(x= train_generator,\n          steps_per_epoch=steps_per_epoch,\n          epochs=num_epochs, verbose = 1)\n\nprint(model.output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate a sequence from the model\ndef generate_seq(model: Sequential, \n                 tokenizer: Tokenizer, \n                 seed: list, \n                 syllable_limit: int):\n    '''\n    Parameters:\n        model: your neural network\n        tokenizer: the keras preprocessing tokenizer\n        seed: [w1, w2, w(n-1)]\n        n_words: generate a sentence of length n syllable\n    Returns: string sentence\n    '''\n    sentence = seed\n    i = 0\n    count_syllables = 0\n    while count_syllables != syllable_limit:\n        # n-1 tokens in sentence\n        curr_tokens = sentence\n        # encode our tokens\n        sequence = tokenizer.texts_to_sequences([curr_tokens])[0]\n        # pre-padding our tokens\n        sequence = np.array(pad_sequences([sequence], maxlen = seq_length-1, padding='pre'))\n        # Convert into 3D\n        embeddings = np.zeros((sequence.shape[0], sequence.shape[1], embedding_size))\n        for i in range(sequence.shape[0]):\n            for j in range(sequence.shape[1]):\n                word = sequence[i,j]\n                embeddings[i, j, :] = index_to_embedding[word]\n        # get probability distribution\n        probs = model.predict(embeddings)[0][2:]\n        # normalize probabilities and get index\n        random_choice = np.random.choice(len(probs),p = probs / np.sum(probs))\n        if random_choice != 0:\n            next_word = tokenizer.index_word[random_choice + 3]\n            # Count new syllables\n            new_count = syllables.estimate(next_word) + count_syllables\n            if next_word not in ['<h>','</h>'] and (new_count <= syllable_limit):\n                sentence.append(next_word)\n                count_syllables = new_count\n        else:\n            sentence = seed\n            count_syllables = 0\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-04-20T00:49:26.448085Z","iopub.status.idle":"2023-04-20T00:49:26.448600Z","shell.execute_reply.started":"2023-04-20T00:49:26.448330Z","shell.execute_reply":"2023-04-20T00:49:26.448356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = # USER QUERY HERE as a list of one string (EX: ['wind'])\ndef generate_haiku(seed):\n    \"\"\"Generates n haikus from a trained language model\n    Parameters:\n      n (int): the number of haikus to generate\n\n    Returns:\n      list: a list containing strings, one per generated sentence\n    \"\"\"\n    haiku = []\n    line_1 = generate_seq(model, tokenizer, seed, 5).split(self.line_begin)[-1]\n    line_2 = generate_seq(model, tokenizer, line_1[-1], 5).split(self.line_begin)[-1]\n    line_3 = generate_seq(model, tokenizer, line_2[-1], 5).split(self.line_begin)[-1]\n    haiku.append(line_1)\n    haiku.append(line_2)\n    haiku.append(line_3)\n    haikus.append(haiku)\n    return haiku","metadata":{"execution":{"iopub.status.busy":"2023-04-19T23:55:05.904335Z","iopub.execute_input":"2023-04-19T23:55:05.905304Z","iopub.status.idle":"2023-04-19T23:55:06.759221Z","shell.execute_reply.started":"2023-04-19T23:55:05.905252Z","shell.execute_reply":"2023-04-19T23:55:06.757687Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 762ms/step\n216\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1304974889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/2871748013.py\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, tokenizer, seed, syllable_limit, NGRAM)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_choice\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Count new syllables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnew_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyllables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcount_syllables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext_word\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<h>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'</h>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_count\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msyllable_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'syllables' is not defined"],"ename":"NameError","evalue":"name 'syllables' is not defined","output_type":"error"}]},{"cell_type":"code","source":"for haiku in generate_haiku(5):\n    for line in haiku:\n        print(line)\n    print('\\n')","metadata":{},"execution_count":null,"outputs":[]}]}